# Server configuration
server:
  port: 8080

# Ollama configuration
spring:
  ai:
    ollama:
      base-url: http://localhost:11434
      chat:
        model: llama2
        options:
          temperature: "0.7"
          top-p: "0.9"

# Logging
logging:
  level:
    root: INFO
    org.springframework.ai: DEBUG
    io.flowinquiry.testcontainers: DEBUG
